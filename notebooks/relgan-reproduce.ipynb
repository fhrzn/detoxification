{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 37\n",
    "# paths\n",
    "COCO_DATASET = '../../TextGAN/dataset/image_coco.txt'\n",
    "JIGSAW_DATASET = ['../../detox/emnlp2021/data/train/train_normal',\n",
    "                  '../../detox/emnlp2021/data/train/train_toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Reproduce RelGAN then adapt with Jigsaw toxic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "def load_file(path):    \n",
    "\n",
    "    with open(path, 'r') as r:\n",
    "        lines = r.read().split('\\n')\n",
    "\n",
    "    return lines\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    '''\n",
    "    :params sentences: list of merged sentences\n",
    "    '''\n",
    "    # tokenize sentences\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "        tokens += word_tokenize(s)\n",
    "\n",
    "    # map id to token\n",
    "    vocab_index_word = dict(enumerate(set(tokens), 3))\n",
    "    vocab_index_word[0] = '<PAD>'\n",
    "    vocab_index_word[1] = '<BOS>'\n",
    "    vocab_index_word[2] = '<EOS>'\n",
    "\n",
    "    # map back token to id\n",
    "    vocab_word_index = {w: i for i, w in vocab_index_word.items()}\n",
    "\n",
    "    return vocab_index_word, vocab_word_index\n",
    "\n",
    "def encode(sentences, vocab, padding=True, max_seq_len='max'):\n",
    "    '''\n",
    "    if padding True, then return tensor with size len(sentences) * max_seq_len\n",
    "    if padding False, then return list of encoded tokens\n",
    "    '''\n",
    "    seq_len = max_seq_len\n",
    "    if max_seq_len == 'max':\n",
    "        seq_len = max([len(x) for x in sentences])\n",
    "\n",
    "    # encode sentences\n",
    "    encoded = []\n",
    "    for s in sentences:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokens = [vocab[token] for token in tokens]\n",
    "        encoded.append(tokens)\n",
    "\n",
    "    if padding:\n",
    "        # create empty tensor with size len(sentences) * max_seq_len\n",
    "        features = torch.zeros((len(encoded), seq_len)).long()\n",
    "        # now fill tensor\n",
    "        for i, row in enumerate(encoded):            \n",
    "            features[i, :len(row)] = torch.tensor(row)[:seq_len]\n",
    "    else:\n",
    "        features = encoded\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_generator_dataset(samples):\n",
    "    inp = torch.zeros_like(samples).long()\n",
    "    target = samples\n",
    "    inp[:, 0] = 1   # the <BOS> token\n",
    "    inp[:, 1:] = target[:, :-1]\n",
    "\n",
    "    return inp, target\n",
    "\n",
    "def prepare_discriminator_dataset(samples_pos, samples_neg):\n",
    "    inp = torch.cat([samples_pos, samples_neg], dim=0).long().detach()\n",
    "    target = torch.ones(len(inp)).float()\n",
    "    target[:len(samples_pos)] = 0\n",
    "\n",
    "    # shuffle\n",
    "    perm = torch.randperm(len(inp))\n",
    "    inp = inp[perm]\n",
    "    target = target[perm]\n",
    "\n",
    "    return inp, target\n",
    "\n",
    "def read_data(samples, samples_neg=None):\n",
    "    \n",
    "    if isinstance(samples, str):\n",
    "        samples = load_file(COCO_DATASET)\n",
    "        vocab_i2w, vocab_w2i = build_vocab(samples)\n",
    "        samples = encode(samples, vocab_w2i, max_seq_len=MAX_SEQ_LEN)\n",
    "    \n",
    "    if samples_neg == None:\n",
    "        inp, tgt = prepare_generator_dataset(samples)\n",
    "    else:\n",
    "\n",
    "        if isinstance(samples_neg, str):\n",
    "            samples_neg = load_file(COCO_DATASET)\n",
    "            vocab_i2w, vocab_w2i = build_vocab(samples_neg)\n",
    "            samples_neg = encode(samples_neg, vocab_w2i, max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "        inp, tgt = prepare_discriminator_dataset(samples, samples_neg)\n",
    "\n",
    "    all_data = [{'input': i, 'target': t} for i, t in zip(inp, tgt)]\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4659"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_w2i, vocab_i2w = build_vocab(load_file(COCO_DATASET))\n",
    "len(vocab_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "class GANDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def get_dataloader(samples, samples_neg=None, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    if samples_neg == None:\n",
    "        return DataLoader(\n",
    "            GANDataset(read_data(samples)),\n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle\n",
    "        )\n",
    "    else:\n",
    "        return DataLoader(\n",
    "            GANDataset(read_data(samples, samples_neg)),\n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle\n",
    "        )\n",
    "\n",
    "def get_random_batch(dataloader):\n",
    "    idx = random.randint(0, len(dataloader) - 1)\n",
    "    return list(dataloader)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genloader = get_dataloader(COCO_DATASET)\n",
    "disloader = get_dataloader(COCO_DATASET, COCO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture (relational memory)\n",
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Constructs a `RelationalMemory` object.\n",
    "    This class is same as the RMC from relational_rnn_models.py, but without language modeling-specific variables.\n",
    "    Args:\n",
    "      mem_slots: The total number of memory slots to use.\n",
    "      head_size: The size of an attention head.\n",
    "      input_size: The size of input per step. i.e. the dimension of each input vector\n",
    "      num_heads: The number of attention heads to use. Defaults to 1.\n",
    "      num_blocks: Number of times to compute attention per time step. Defaults\n",
    "        to 1.\n",
    "      forget_bias: Bias to use for the forget gate, assuming we are using\n",
    "        some form of gating. Defaults to 1.\n",
    "      input_bias: Bias to use for the input gate, assuming we are using\n",
    "        some form of gating. Defaults to 0.\n",
    "      gate_style: Whether to use per-element gating ('unit'),\n",
    "        per-memory slot gating ('memory'), or no gating at all (None).\n",
    "        Defaults to `unit`.\n",
    "      attention_mlp_layers: Number of layers to use in the post-attention\n",
    "        MLP. Defaults to 2.\n",
    "      key_size: Size of vector to use for key & query vectors in the attention\n",
    "        computation. Defaults to None, in which case we use `head_size`.\n",
    "      name: Name of the module.\n",
    "\n",
    "      # NEW flag for this class\n",
    "      return_all_outputs: Whether the model returns outputs for each step (like seq2seq) or only the final output.\n",
    "    Raises:\n",
    "      ValueError: gate_style not one of [None, 'memory', 'unit'].\n",
    "      ValueError: num_blocks is < 1.\n",
    "      ValueError: attention_mlp_layers is < 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mem_slots, head_size, input_size, num_heads=1, num_blocks=1, forget_bias=1., input_bias=0.,\n",
    "                 gate_style='unit', attention_mlp_layers=2, key_size=None, return_all_outputs=False):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "\n",
    "        ########## generic parameters for RMC ##########\n",
    "        self.mem_slots = mem_slots\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_size = self.head_size * self.num_heads\n",
    "\n",
    "        # a new fixed params needed for pytorch port of RMC\n",
    "        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n",
    "        # so if the mem_slots = 1, this value is 2\n",
    "        self.mem_slots_plus_input = self.mem_slots + 1\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >=1. Got: {}.'.format(num_blocks))\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self.gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self.attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self.key_size = key_size if key_size else self.head_size\n",
    "\n",
    "        ########## parameters for multihead attention ##########\n",
    "        # value_size is same as head_size\n",
    "        self.value_size = self.head_size\n",
    "        # total size for query-key-value\n",
    "        self.qkv_size = 2 * self.key_size + self.value_size\n",
    "        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n",
    "\n",
    "        # each head has qkv_sized linear projector\n",
    "        # just using one big param is more efficient, rather than this line\n",
    "        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n",
    "        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n",
    "        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n",
    "\n",
    "        # used for attend_over_memory function\n",
    "        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n",
    "        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "\n",
    "        ########## parameters for initial embedded input projection ##########\n",
    "        self.input_size = input_size\n",
    "        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n",
    "\n",
    "        ########## parameters for gating ##########\n",
    "        self.num_gates = 2 * self.calculate_gate_size()\n",
    "        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        # trainable scalar gate bias tensors\n",
    "        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n",
    "        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n",
    "\n",
    "        ########## number of outputs returned #####\n",
    "        self.return_all_outputs = return_all_outputs\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "        # needed for truncated BPTT, called at every batch forward pass\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def initial_state(self, batch_size, trainable=False):\n",
    "        \"\"\"\n",
    "        Creates the initial memory.\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size\n",
    "        (batch_size, self.mem_slots, self.mem_size).\n",
    "        Args:\n",
    "          batch_size: The size of the batch.\n",
    "          trainable: Whether the initial state is trainable. This is always True.\n",
    "        Returns:\n",
    "          init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self.mem_slots, self.mem_size).\n",
    "        \"\"\"\n",
    "        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "        # pad the matrix with zeros\n",
    "        if self.mem_size > self.mem_slots:\n",
    "            difference = self.mem_size - self.mem_slots\n",
    "            pad = torch.zeros((batch_size, self.mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], -1)\n",
    "\n",
    "        # truncation. take the first 'self.mem_size' components\n",
    "        elif self.mem_size < self.mem_slots:\n",
    "            init_state = init_state[:, :, :self.mem_size]\n",
    "\n",
    "        return init_state\n",
    "\n",
    "    def multihead_attention(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multi-head attention from 'Attention is All You Need'.\n",
    "        Implementation of the attention mechanism from\n",
    "        https://arxiv.org/abs/1706.03762.\n",
    "        Args:\n",
    "          memory: Memory tensor to perform attention on.\n",
    "        Returns:\n",
    "          new_memory: New memory tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, a simple linear projection is used to construct queries\n",
    "        qkv = self.qkv_projector(memory)\n",
    "        # apply layernorm for every dim except the batch dim\n",
    "        qkv = self.qkv_layernorm(qkv)\n",
    "\n",
    "        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n",
    "        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n",
    "        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n",
    "        mem_slots = memory.shape[1]  # denoted as N\n",
    "\n",
    "        # split the qkv to multiple heads H\n",
    "        # [B, N, F] => [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n",
    "\n",
    "        # [B, N, H, F/H] => [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "\n",
    "        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n",
    "        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n",
    "\n",
    "        # scale q with d_k, the dimensionality of the key vectors\n",
    "        q = q * (self.key_size ** -0.5)\n",
    "\n",
    "        # make it [B, H, N, N]\n",
    "        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "\n",
    "        # output is [B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "\n",
    "        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n",
    "\n",
    "        return new_memory\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [self.mem_slots, self.mem_size]\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.mem_slots * self.mem_size\n",
    "\n",
    "    def calculate_gate_size(self):\n",
    "        \"\"\"\n",
    "        Calculate the gate size from the gate_style.\n",
    "        Returns:\n",
    "          The per sample, per head parameter size of each gate.\n",
    "        \"\"\"\n",
    "        if self.gate_style == 'unit':\n",
    "            return self.mem_size\n",
    "        elif self.gate_style == 'memory':\n",
    "            return 1\n",
    "        else:  # self.gate_style == None\n",
    "            return 0\n",
    "\n",
    "    def create_gates(self, inputs, memory):\n",
    "        \"\"\"\n",
    "        Create input and forget gates for this step using `inputs` and `memory`.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: The current state of memory.\n",
    "        Returns:\n",
    "          input_gate: A LSTM-like insert gate.\n",
    "          forget_gate: A LSTM-like forget gate.\n",
    "        \"\"\"\n",
    "        # We'll create the input and forget gates at once. Hence, calculate double\n",
    "        # the gate size.\n",
    "\n",
    "        # equation 8: since there is no output gate, h is just a tanh'ed m\n",
    "        memory = torch.tanh(memory)\n",
    "\n",
    "        # sonnet uses this, but i think it assumes time step of 1 for all cases\n",
    "        # if inputs is (B, T, features) where T > 1, this gets incorrect\n",
    "        # inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "        # fixed implementation\n",
    "        if len(inputs.shape) == 3:\n",
    "            if inputs.shape[1] > 1:\n",
    "                raise ValueError(\n",
    "                    \"input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1\")\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # matmul for equation 4 and 5\n",
    "            # there is no output gate, so equation 6 is not implemented\n",
    "            gate_inputs = self.input_gate_projector(inputs)\n",
    "            gate_inputs = gate_inputs.unsqueeze(dim=1)\n",
    "            gate_memory = self.memory_gate_projector(memory)\n",
    "        else:\n",
    "            raise ValueError(\"input shape of create_gate function is 2, expects 3\")\n",
    "\n",
    "        # this completes the equation 4 and 5\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "        assert input_gate.shape[2] == forget_gate.shape[2]\n",
    "\n",
    "        # to be used for equation 7\n",
    "        input_gate = torch.sigmoid(input_gate + self.input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "    def attend_over_memory(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multiheaded attention over `memory`.\n",
    "            Args:\n",
    "              memory: Current relational memory.\n",
    "            Returns:\n",
    "              The attended-over memory.\n",
    "        \"\"\"\n",
    "        for _ in range(self.num_blocks):\n",
    "            attended_memory = self.multihead_attention(memory)\n",
    "\n",
    "            # Add a skip connection to the multiheaded attention's input.\n",
    "            memory = self.attended_memory_layernorm(memory + attended_memory)\n",
    "\n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            attention_mlp = memory\n",
    "            for i, l in enumerate(self.attention_mlp):\n",
    "                attention_mlp = self.attention_mlp[i](attention_mlp)\n",
    "                attention_mlp = F.relu(attention_mlp)\n",
    "            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n",
    "\n",
    "        return memory\n",
    "\n",
    "    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        \"\"\"\n",
    "        Forward step of the relational memory core.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: Memory output from the previous time step.\n",
    "          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n",
    "            of matrices. Default to False, in which case the input is flattened\n",
    "            into a vector.\n",
    "        Returns:\n",
    "          output: This time step's output.\n",
    "          next_memory: The next version of memory to use.\n",
    "        \"\"\"\n",
    "\n",
    "        if treat_input_as_matrix:\n",
    "            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n",
    "            inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n",
    "            # apply linear layer for dim 2\n",
    "            inputs_reshape = self.input_projector(inputs)\n",
    "        else:\n",
    "            # keep (Batch, ...) dim (0), flatten starting from dim 1\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # apply linear layer for dim 1\n",
    "            inputs = self.input_projector(inputs)\n",
    "            # unsqueeze the time step to dim 1\n",
    "            inputs_reshape = inputs.unsqueeze(dim=1)\n",
    "\n",
    "        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "        next_memory = self.attend_over_memory(memory_plus_input)\n",
    "\n",
    "        # cut out the concatenated input vectors from the original memory slots\n",
    "        n = inputs_reshape.shape[1]\n",
    "        next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "        if self.gate_style == 'unit' or self.gate_style == 'memory':\n",
    "            # these gates are sigmoid-applied ones for equation 7\n",
    "            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n",
    "            # equation 7 calculation\n",
    "            next_memory = input_gate * torch.tanh(next_memory)\n",
    "            next_memory += forget_gate * memory\n",
    "\n",
    "        output = next_memory.view(next_memory.shape[0], -1)\n",
    "\n",
    "        return output, next_memory\n",
    "\n",
    "    def forward(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "\n",
    "        # for loop implementation of (entire) recurrent forward pass of the model\n",
    "        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n",
    "        # so the concatenated logits are [seq * batch, vocab]\n",
    "\n",
    "        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n",
    "\n",
    "        # memory = self.repackage_hidden(memory)\n",
    "        logit = 0\n",
    "        logits = []\n",
    "        # shape[1] is seq_lenth T\n",
    "        for idx_step in range(inputs.shape[1]):\n",
    "            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n",
    "            logits.append(logit.unsqueeze(1))\n",
    "        logits = torch.cat(logits, dim=1)\n",
    "\n",
    "        if self.return_all_outputs:\n",
    "            return logits, memory\n",
    "        else:\n",
    "            return logit.unsqueeze(1), memory\n",
    "\n",
    "# ########## DEBUG: unit test code ##########\n",
    "# input_size = 32\n",
    "# seq_length = 20\n",
    "# batch_size = 32\n",
    "# num_tokens = 5000\n",
    "# model = RelationalMemory(mem_slots=1, head_size=512, input_size=input_size, num_heads=2)\n",
    "# model_memory = model.initial_state(batch_size=batch_size)\n",
    "#\n",
    "# # random input\n",
    "# random_input = torch.randn((32, seq_length, input_size))\n",
    "# # random targets\n",
    "# random_targets = torch.randn((32, seq_length, input_size))\n",
    "#\n",
    "# # take a one step forward\n",
    "# logit, next_memory = model(random_input, model_memory)\n",
    "# print(next_memory.shape)\n",
    "# print(logit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture (LSTM/RMC Generator)\n",
    "class RelGAN(nn.Module):\n",
    "    def __init__(self, mem_slots, num_heads, head_size, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, device=DEVICE):    \n",
    "        super(RelGAN, self).__init__()\n",
    "\n",
    "        # ---------------- #\n",
    "        # model properties #\n",
    "        # ---------------- #\n",
    "        self.name = 'RelGAN'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.mem_slots = mem_slots\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.temperature = 1.0\n",
    "        self.theta = None\n",
    "        self.device = device\n",
    "\n",
    "        # ------------ #\n",
    "        # model layers #\n",
    "        # ------------ #\n",
    "        # LSTM\n",
    "        # self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        # self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # RMC\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.hidden_size = mem_slots * num_heads * head_size\n",
    "        self.lstm = RelationalMemory(mem_slots=mem_slots, head_size=head_size, input_size=embedding_dim,\n",
    "                                     num_heads=num_heads, return_all_outputs=True)\n",
    "        self.lstm2out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, h, need_hidden=False):\n",
    "        emb = self.embeddings(x)\n",
    "        if len(x.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim        \n",
    "        out, hidden = self.lstm(emb, h)  # out: batch_size * seq_len * hidden_dim        \n",
    "        out = out.contiguous().view(-1, self.hidden_size)  # out: (batch_size * len) * hidden_dim        \n",
    "        out = self.lstm2out(out)  # batch_size * seq_len * vocab_size        \n",
    "        out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)        \n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "\n",
    "    def step(self, x, h):\n",
    "        '''\n",
    "        RelGAN step forward\n",
    "        :param inp: [batch_size]\n",
    "        :param hidden: memory size\n",
    "        :return: pred, hidden, next_token, next_token_onehot, next_o\n",
    "            - pred: batch_size * vocab_size, use for adversarial training backward\n",
    "            - hidden: next hidden\n",
    "            - next_token: [batch_size], next sentence token\n",
    "            - next_token_onehot: batch_size * vocab_size, not used yet\n",
    "            - next_o: batch_size * vocab_size, not used yet\n",
    "        '''        \n",
    "        emb = self.embeddings(x).unsqueeze(1)\n",
    "        out, hidden = self.lstm(emb, h)\n",
    "        gumbel_t = self.add_gumbel(self.lstm2out(out.squeeze(1)))\n",
    "        next_token = torch.argmax(gumbel_t, dim=1).detach()\n",
    "        # next_token_onehot = F.one_hot(next_token, cfg.vocab_size).float()  # not used yet\n",
    "        next_token_onehot = None\n",
    "\n",
    "        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n",
    "        # next_o = torch.sum(next_token_onehot * pred, dim=1)  # not used yet\n",
    "        next_o = None\n",
    "\n",
    "        return pred, hidden, next_token, next_token_onehot, next_o\n",
    "\n",
    "\n",
    "    def sample(self, num_samples, batch_size, one_hot=False, start_letter=1):\n",
    "        \"\"\"\n",
    "        Sample from RelGAN Generator\n",
    "        - one_hot: if return pred of RelGAN, used for adversarial training\n",
    "        :return:\n",
    "            - all_preds: batch_size * seq_len * vocab_size, only use for a batch\n",
    "            - samples: all samples\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len, device=self.device).long()\n",
    "        if one_hot:\n",
    "            all_preds = torch.zeros(batch_size, self.max_seq_len, self.vocab_size, device=self.device)\n",
    "\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size).to(self.device)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size).to(self.device)            \n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                pred, hidden, next_token, _, _ = self.step(inp, hidden)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token\n",
    "                if one_hot:\n",
    "                    all_preds[:, i] = pred\n",
    "                inp = next_token\n",
    "        samples = samples[:num_samples]  # num_samples * seq_len\n",
    "\n",
    "        if one_hot:\n",
    "            return all_preds  # batch_size * seq_len * vocab_size\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size=BATCH_SIZE):\n",
    "        \"\"\"init RMC memory\"\"\"\n",
    "        memory = self.lstm.initial_state(batch_size)\n",
    "        memory = self.lstm.repackage_hidden(memory)  # detch memory at first        \n",
    "        return memory.to(self.device)\n",
    "\n",
    "    \n",
    "    def add_gumbel(self, o_t, eps=1e-10):\n",
    "        \"\"\"Add o_t by a vector sampled from Gumbel(0,1)\"\"\"\n",
    "        u = torch.zeros(o_t.size(), device=self.device)\n",
    "        \n",
    "        u.uniform_(0, 1)\n",
    "        g_t = -torch.log(-torch.log(u + eps) + eps)\n",
    "        gumbel_t = o_t + g_t\n",
    "        return gumbel_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_SLOTS = 1\n",
    "NUM_HEADS = 2\n",
    "HEAD_SIZE = 256\n",
    "GEN_EMBED_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RelGAN(MEM_SLOTS, NUM_HEADS, HEAD_SIZE, embedding_dim=GEN_EMBED_DIM, \n",
    "             hidden_dim=GEN_HIDDEN_DIM, vocab_size=len(vocab_w2i), \n",
    "             max_seq_len=MAX_SEQ_LEN, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture discriminator\n",
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len, num_rep, vocab_size, padding_idx,\n",
    "                 filter_sizes, num_filters, dropout=0.25, device=DEVICE):\n",
    "    # def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "\n",
    "        # ---------------- #\n",
    "        # model properties #\n",
    "        # ---------------- #\n",
    "        self.name = 'CNNDiscriminator'\n",
    "        self.embed_size = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.emb_dim_single = int(embed_dim / num_rep)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx        \n",
    "        \n",
    "        # ------------ #\n",
    "        # model layers #\n",
    "        # ------------ #\n",
    "        self.embeddings = nn.Linear(vocab_size, embed_dim, bias=False)        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, self.emb_dim_single), stride=(1, self.emb_dim_single)) for (n, f) in \n",
    "            zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 100)\n",
    "        self.out2logits = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embeddings(x).unsqueeze(1)\n",
    "\n",
    "        convs = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pools = [F.max_pool2d(con, (con.size(2), 1)).squeeze(2) for con in convs]\n",
    "        pred = torch.cat(pools, 1)\n",
    "        pred = pred.permute(0, 2, 1).contiguous().view(-1, self.feature_dim)\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred\n",
    "\n",
    "        pred = self.feature2out(self.dropout(pred))\n",
    "        logits = self.out2logits(pred).squeeze(1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIS_EMBED_DIM = 64\n",
    "NUM_REP = 64\n",
    "\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "num_filters = [300, 300, 300, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = CNNDiscriminator(DIS_EMBED_DIM, MAX_SEQ_LEN, NUM_REP, len(vocab_w2i), padding_idx=2,\n",
    "                        filter_sizes=filter_sizes, num_filters=num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9f7e278d3c48998b63dc9e5d326e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 | Loss: 1.7589457589349928\n",
      "Epoch: 2/3 | Loss: 1.1059965510277232\n",
      "Epoch: 3/3 | Loss: 0.9907671475106743\n"
     ]
    }
   ],
   "source": [
    "# pretraining generator\n",
    "gen_criterion = nn.NLLLoss()\n",
    "gen_optim = Adam(gen.parameters(), lr=0.01)\n",
    "gen_pretrain_epochs = 150\n",
    "gen_clip_norm = 5.0\n",
    "gen_loss_min = torch.inf\n",
    "\n",
    "gen.to(DEVICE)\n",
    "\n",
    "epochloop = tqdm(range(gen_pretrain_epochs), position=0, desc='Training...', leave=True)\n",
    "\n",
    "for e in epochloop:\n",
    "    \n",
    "    gen.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(genloader):\n",
    "        feature, target = batch['input'].to(DEVICE), batch['target'].to(DEVICE)\n",
    "        hidden = gen.init_hidden(batch_size=len(feature)).to(DEVICE)    # use length of feature as batch_size to handle different last batch total item\n",
    "\n",
    "        pred = gen(feature, hidden)\n",
    "\n",
    "        loss = gen_criterion(pred, target.view(-1))\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), gen_clip_norm)\n",
    "        gen_optim.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        epochloop.set_postfix_str(f'Batch: {i+1}/{len(genloader)} | Loss: {train_loss/(i+1):.3f}')\n",
    "\n",
    "    avg_loss = train_loss / len(genloader)\n",
    "\n",
    "    print(f'Epoch: {e+1}/{gen_pretrain_epochs} | Loss: {avg_loss}')\n",
    "    if avg_loss <= gen_loss_min:\n",
    "        torch.save(gen.state_dict(), f'../models/gan/pretrain_{gen.name}.pt')\n",
    "        gen_loss_min = avg_loss\n",
    "    else:\n",
    "        print(f'[WARN] Loss didn\\'t improving ({gen_loss_min:.4f} --> {avg_loss:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def get_losses(d_out_real, d_out_fake, loss_type='JS'):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if loss_type == 'standard':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = bce_loss(d_out_fake, torch.ones_like(d_out_fake))\n",
    "\n",
    "    elif loss_type == 'JS':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -d_loss_fake\n",
    "\n",
    "    elif loss_type == 'KL':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = torch.mean(-d_out_fake)\n",
    "\n",
    "    elif loss_type == 'hinge':\n",
    "        d_loss_real = torch.mean(nn.ReLU(1.0 - d_out_real))\n",
    "        d_loss_fake = torch.mean(nn.ReLU(1,0 + d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -torch.mean(d_out_fake)\n",
    "\n",
    "    elif loss_type == 'rsgan':\n",
    "        d_loss = bce_loss(d_out_real - d_out_fake, torch.ones_like(d_out_real))\n",
    "        g_loss = bce_loss(d_out_fake - d_out_real, torch.ones_like(d_out_fake))\n",
    "\n",
    "    return g_loss, d_loss\n",
    "\n",
    "\n",
    "def get_fixed_temperature(temper, i, N, adapt='exp'):\n",
    "    \"\"\"A function to set up different temperature control policies\"\"\"\n",
    "    N = 5000\n",
    "\n",
    "    if adapt == 'no':\n",
    "        temper_var_np = 1.0  # no increase, origin: temper\n",
    "    elif adapt == 'lin':\n",
    "        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n",
    "    elif adapt == 'exp':\n",
    "        temper_var_np = temper ** (i / N)  # exponential increase\n",
    "    elif adapt == 'log':\n",
    "        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n",
    "    elif adapt == 'sigmoid':\n",
    "        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n",
    "    elif adapt == 'quad':\n",
    "        temper_var_np = (temper - 1) / (N - 1) ** 2 * i ** 2 + 1\n",
    "    elif adapt == 'sqrt':\n",
    "        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown adapt type!\")\n",
    "\n",
    "    return temper_var_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial training\n",
    "adv_train_epoch = 2000\n",
    "adv_gen_step = 1\n",
    "adv_disc_step = 5\n",
    "gen_adv_optim = Adam(gen.parameters(), lr=1e-4)\n",
    "disc_adv_optim = Adam(disc.parameters(), lr=1e-4)\n",
    "adv_clip_norm = 5.0\n",
    "loss_type = 'rsgan'\n",
    "\n",
    "adv_epochloop = tqdm(range(adv_train_epoch))\n",
    "\n",
    "for e in adv_epochloop:\n",
    "    # adv train generator\n",
    "    adv_gen_loss = 0\n",
    "    for i in range(adv_gen_step):\n",
    "        # get random real sample\n",
    "        real_sample = get_random_batch(genloader)['target'].to(DEVICE)\n",
    "        gen_sample = gen.sample(BATCH_SIZE, BATCH_SIZE, one_hot=True).to(DEVICE)\n",
    "        real_sample = F.one_hot(real_sample, len(vocab_w2i)).float()\n",
    "\n",
    "        # train\n",
    "        d_out_real = disc(real_sample)\n",
    "        d_out_fake = disc(gen_sample)\n",
    "        g_loss, _ = get_losses(d_out_real, d_out_fake, loss_type=loss_type)\n",
    "\n",
    "        # optimize\n",
    "        gen_adv_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), adv_clip_norm)\n",
    "        adv_gen_loss += g_loss.item()\n",
    "\n",
    "        adv_epochloop.set_description(f'Generator step: {i+1}/{adv_gen_step} | g_loss: {(g_loss/(i+1)):.3f}')        \n",
    "\n",
    "    adv_gen_loss = adv_gen_loss / adv_gen_step if adv_gen_step != 0 else 0\n",
    "\n",
    "    # adv train discriminator\n",
    "    adv_disc_loss = 0\n",
    "    for i in range(adv_disc_step):\n",
    "        # get random real sample\n",
    "        real_sample = get_random_batch(genloader)['target'].to(DEVICE)\n",
    "        gen_sample = gen.sample(BATCH_SIZE, BATCH_SIZE, one_hot=True).to(DEVICE)\n",
    "        real_sample = F.one_hot(real_sample, len(vocab_w2i)).float()\n",
    "\n",
    "        # train\n",
    "        d_out_real = disc(real_sample)\n",
    "        d_out_fake = disc(gen_sample)\n",
    "        _, d_loss = get_losses(d_out_real, d_out_fake, loss_type=loss_type)\n",
    "\n",
    "        # optimize\n",
    "        disc_adv_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        torch.nn.utils.clip_grad(disc.parameters(), adv_clip_norm)\n",
    "        adv_disc_loss += d_loss.item()\n",
    "\n",
    "        adv_epochloop.set_description(f'Discriminator step: {i+1}/{adv_disc_step} | d_loss: {(d_loss/(i+1)):.3f}')\n",
    "\n",
    "    adv_disc_loss = adv_disc_loss / adv_disc_step if adv_disc_step != 0 else 0\n",
    "\n",
    "    # update generator temperature\n",
    "    gen.temperature = get_fixed_temperature(1, e, adv_train_epoch, adapt='exp')\n",
    "    \n",
    "    print(f'[ADV] Epoch: {e}/{adv_train_epoch} | g_loss: {adv_gen_loss:.4f}, d_loss: {adv_disc_loss:.4f}, temperature: {gen.temperature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8564a27cb82d73423f8ef7649afe412fe88be26d8d7a10840ebe1fcfca8dcfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
