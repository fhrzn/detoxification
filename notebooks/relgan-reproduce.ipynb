{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "MAX_SEQ_LEN = 37\n",
    "# paths\n",
    "COCO_DATASET = '../../TextGAN/dataset/image_coco.txt'\n",
    "JIGSAW_DATASET = ['../../detox/emnlp2021/data/train/train_normal',\n",
    "                  '../../detox/emnlp2021/data/train/train_toxic',]\n",
    "                #   '../../detox/emnlp2021/data/test/test_10k_toxic',\n",
    "                #   '../../detox/emnlp2021/data/test/test_10k_toxic',\n",
    "                #   '../../detox/emnlp2021/data/dev/dev_toxic',\n",
    "                #   '../../detox/emnlp2021/data/dev/dev_toxic',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Reproduce RelGAN then adapt with Jigsaw toxic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "def load_file(path):    \n",
    "\n",
    "    if isinstance(path, str):\n",
    "        path = [path]\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for p in path:\n",
    "        with open(p, 'r') as r:\n",
    "            lines += r.read().split('\\n')\n",
    "\n",
    "    return lines\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    '''\n",
    "    :params sentences: list of merged sentences\n",
    "    '''\n",
    "    # tokenize sentences\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "        tokens += word_tokenize(s)\n",
    "\n",
    "    # map id to token\n",
    "    vocab_index_word = dict(enumerate(set(tokens), 3))\n",
    "    vocab_index_word[0] = '<PAD>'\n",
    "    vocab_index_word[1] = '<BOS>'\n",
    "    vocab_index_word[2] = '<EOS>'\n",
    "\n",
    "    # map back token to id\n",
    "    vocab_word_index = {w: i for i, w in vocab_index_word.items()}\n",
    "\n",
    "    return vocab_index_word, vocab_word_index\n",
    "\n",
    "def encode(sentences, vocab, padding=True, max_seq_len='max'):\n",
    "    '''\n",
    "    if padding True, then return tensor with size len(sentences) * max_seq_len\n",
    "    if padding False, then return list of encoded tokens\n",
    "    '''\n",
    "    seq_len = max_seq_len\n",
    "    if max_seq_len == 'max':\n",
    "        seq_len = max([len(x) for x in sentences])\n",
    "\n",
    "    # encode sentences\n",
    "    encoded = []\n",
    "    for s in sentences:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokens = [vocab[token] for token in tokens]\n",
    "        encoded.append(tokens)\n",
    "\n",
    "    if padding:\n",
    "        # create empty tensor with size len(sentences) * max_seq_len\n",
    "        features = torch.zeros((len(encoded), seq_len)).long()\n",
    "        # now fill tensor\n",
    "        for i, row in enumerate(encoded):            \n",
    "            features[i, :len(row)] = torch.tensor(row)[:seq_len]\n",
    "    else:\n",
    "        features = encoded\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_generator_dataset(samples):\n",
    "    inp = torch.zeros_like(samples).long()\n",
    "    target = samples\n",
    "    inp[:, 0] = 1   # the <BOS> token\n",
    "    inp[:, 1:] = target[:, :-1]\n",
    "\n",
    "    return inp, target\n",
    "\n",
    "def prepare_discriminator_dataset(samples_pos, samples_neg):\n",
    "    inp = torch.cat([samples_pos, samples_neg], dim=0).long().detach()\n",
    "    target = torch.ones(len(inp)).float()\n",
    "    target[:len(samples_pos)] = 0\n",
    "\n",
    "    # shuffle\n",
    "    perm = torch.randperm(len(inp))\n",
    "    inp = inp[perm]\n",
    "    target = target[perm]\n",
    "\n",
    "    return inp, target\n",
    "\n",
    "def read_data(samples, samples_neg=None):\n",
    "    \n",
    "    if isinstance(samples, str):\n",
    "        samples = load_file(COCO_DATASET)\n",
    "        vocab_i2w, vocab_w2i = build_vocab(samples)\n",
    "        samples = encode(samples, vocab_w2i, max_seq_len=MAX_SEQ_LEN)\n",
    "    \n",
    "    if samples_neg == None:\n",
    "        inp, tgt = prepare_generator_dataset(samples)\n",
    "    else:\n",
    "\n",
    "        if isinstance(samples_neg, str):\n",
    "            samples_neg = load_file(COCO_DATASET)\n",
    "            vocab_i2w, vocab_w2i = build_vocab(samples_neg)\n",
    "            samples_neg = encode(samples_neg, vocab_w2i, max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "        inp, tgt = prepare_discriminator_dataset(samples, samples_neg)\n",
    "\n",
    "    all_data = [{'input': i, 'target': t} for i, t in zip(inp, tgt)]\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_w2i, vocab_i2w = build_vocab(load_file(COCO_DATASET))\n",
    "VOCAB_SIZE = len(vocab_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "class GANDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def get_dataloader(samples, samples_neg=None, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    if samples_neg == None:\n",
    "        return DataLoader(\n",
    "            GANDataset(read_data(samples)),\n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle\n",
    "        )\n",
    "    else:\n",
    "        return DataLoader(\n",
    "            GANDataset(read_data(samples, samples_neg)),\n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle\n",
    "        )\n",
    "\n",
    "def get_random_batch(dataloader):\n",
    "    idx = random.randint(0, len(dataloader) - 1)\n",
    "    return list(dataloader)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genloader = get_dataloader(COCO_DATASET)\n",
    "disloader = get_dataloader(COCO_DATASET, COCO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture (relational memory)\n",
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Constructs a `RelationalMemory` object.\n",
    "    This class is same as the RMC from relational_rnn_models.py, but without language modeling-specific variables.\n",
    "    Args:\n",
    "      mem_slots: The total number of memory slots to use.\n",
    "      head_size: The size of an attention head.\n",
    "      input_size: The size of input per step. i.e. the dimension of each input vector\n",
    "      num_heads: The number of attention heads to use. Defaults to 1.\n",
    "      num_blocks: Number of times to compute attention per time step. Defaults\n",
    "        to 1.\n",
    "      forget_bias: Bias to use for the forget gate, assuming we are using\n",
    "        some form of gating. Defaults to 1.\n",
    "      input_bias: Bias to use for the input gate, assuming we are using\n",
    "        some form of gating. Defaults to 0.\n",
    "      gate_style: Whether to use per-element gating ('unit'),\n",
    "        per-memory slot gating ('memory'), or no gating at all (None).\n",
    "        Defaults to `unit`.\n",
    "      attention_mlp_layers: Number of layers to use in the post-attention\n",
    "        MLP. Defaults to 2.\n",
    "      key_size: Size of vector to use for key & query vectors in the attention\n",
    "        computation. Defaults to None, in which case we use `head_size`.\n",
    "      name: Name of the module.\n",
    "\n",
    "      # NEW flag for this class\n",
    "      return_all_outputs: Whether the model returns outputs for each step (like seq2seq) or only the final output.\n",
    "    Raises:\n",
    "      ValueError: gate_style not one of [None, 'memory', 'unit'].\n",
    "      ValueError: num_blocks is < 1.\n",
    "      ValueError: attention_mlp_layers is < 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mem_slots, head_size, input_size, num_heads=1, num_blocks=1, forget_bias=1., input_bias=0.,\n",
    "                 gate_style='unit', attention_mlp_layers=2, key_size=None, return_all_outputs=False):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "\n",
    "        ########## generic parameters for RMC ##########\n",
    "        self.mem_slots = mem_slots\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_size = self.head_size * self.num_heads\n",
    "\n",
    "        # a new fixed params needed for pytorch port of RMC\n",
    "        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n",
    "        # so if the mem_slots = 1, this value is 2\n",
    "        self.mem_slots_plus_input = self.mem_slots + 1\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >=1. Got: {}.'.format(num_blocks))\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self.gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self.attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self.key_size = key_size if key_size else self.head_size\n",
    "\n",
    "        ########## parameters for multihead attention ##########\n",
    "        # value_size is same as head_size\n",
    "        self.value_size = self.head_size\n",
    "        # total size for query-key-value\n",
    "        self.qkv_size = 2 * self.key_size + self.value_size\n",
    "        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n",
    "\n",
    "        # each head has qkv_sized linear projector\n",
    "        # just using one big param is more efficient, rather than this line\n",
    "        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n",
    "        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n",
    "        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n",
    "\n",
    "        # used for attend_over_memory function\n",
    "        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n",
    "        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "\n",
    "        ########## parameters for initial embedded input projection ##########\n",
    "        self.input_size = input_size\n",
    "        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n",
    "\n",
    "        ########## parameters for gating ##########\n",
    "        self.num_gates = 2 * self.calculate_gate_size()\n",
    "        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        # trainable scalar gate bias tensors\n",
    "        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n",
    "        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n",
    "\n",
    "        ########## number of outputs returned #####\n",
    "        self.return_all_outputs = return_all_outputs\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "        # needed for truncated BPTT, called at every batch forward pass\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def initial_state(self, batch_size, trainable=False):\n",
    "        \"\"\"\n",
    "        Creates the initial memory.\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size\n",
    "        (batch_size, self.mem_slots, self.mem_size).\n",
    "        Args:\n",
    "          batch_size: The size of the batch.\n",
    "          trainable: Whether the initial state is trainable. This is always True.\n",
    "        Returns:\n",
    "          init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self.mem_slots, self.mem_size).\n",
    "        \"\"\"\n",
    "        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "        # pad the matrix with zeros\n",
    "        if self.mem_size > self.mem_slots:\n",
    "            difference = self.mem_size - self.mem_slots\n",
    "            pad = torch.zeros((batch_size, self.mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], -1)\n",
    "\n",
    "        # truncation. take the first 'self.mem_size' components\n",
    "        elif self.mem_size < self.mem_slots:\n",
    "            init_state = init_state[:, :, :self.mem_size]\n",
    "\n",
    "        return init_state\n",
    "\n",
    "    def multihead_attention(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multi-head attention from 'Attention is All You Need'.\n",
    "        Implementation of the attention mechanism from\n",
    "        https://arxiv.org/abs/1706.03762.\n",
    "        Args:\n",
    "          memory: Memory tensor to perform attention on.\n",
    "        Returns:\n",
    "          new_memory: New memory tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, a simple linear projection is used to construct queries\n",
    "        qkv = self.qkv_projector(memory)\n",
    "        # apply layernorm for every dim except the batch dim\n",
    "        qkv = self.qkv_layernorm(qkv)\n",
    "\n",
    "        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n",
    "        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n",
    "        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n",
    "        mem_slots = memory.shape[1]  # denoted as N\n",
    "\n",
    "        # split the qkv to multiple heads H\n",
    "        # [B, N, F] => [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n",
    "\n",
    "        # [B, N, H, F/H] => [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "\n",
    "        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n",
    "        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n",
    "\n",
    "        # scale q with d_k, the dimensionality of the key vectors\n",
    "        q = q * (self.key_size ** -0.5)\n",
    "\n",
    "        # make it [B, H, N, N]\n",
    "        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "\n",
    "        # output is [B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "\n",
    "        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n",
    "\n",
    "        return new_memory\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [self.mem_slots, self.mem_size]\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.mem_slots * self.mem_size\n",
    "\n",
    "    def calculate_gate_size(self):\n",
    "        \"\"\"\n",
    "        Calculate the gate size from the gate_style.\n",
    "        Returns:\n",
    "          The per sample, per head parameter size of each gate.\n",
    "        \"\"\"\n",
    "        if self.gate_style == 'unit':\n",
    "            return self.mem_size\n",
    "        elif self.gate_style == 'memory':\n",
    "            return 1\n",
    "        else:  # self.gate_style == None\n",
    "            return 0\n",
    "\n",
    "    def create_gates(self, inputs, memory):\n",
    "        \"\"\"\n",
    "        Create input and forget gates for this step using `inputs` and `memory`.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: The current state of memory.\n",
    "        Returns:\n",
    "          input_gate: A LSTM-like insert gate.\n",
    "          forget_gate: A LSTM-like forget gate.\n",
    "        \"\"\"\n",
    "        # We'll create the input and forget gates at once. Hence, calculate double\n",
    "        # the gate size.\n",
    "\n",
    "        # equation 8: since there is no output gate, h is just a tanh'ed m\n",
    "        memory = torch.tanh(memory)\n",
    "\n",
    "        # sonnet uses this, but i think it assumes time step of 1 for all cases\n",
    "        # if inputs is (B, T, features) where T > 1, this gets incorrect\n",
    "        # inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "        # fixed implementation\n",
    "        if len(inputs.shape) == 3:\n",
    "            if inputs.shape[1] > 1:\n",
    "                raise ValueError(\n",
    "                    \"input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1\")\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # matmul for equation 4 and 5\n",
    "            # there is no output gate, so equation 6 is not implemented\n",
    "            gate_inputs = self.input_gate_projector(inputs)\n",
    "            gate_inputs = gate_inputs.unsqueeze(dim=1)\n",
    "            gate_memory = self.memory_gate_projector(memory)\n",
    "        else:\n",
    "            raise ValueError(\"input shape of create_gate function is 2, expects 3\")\n",
    "\n",
    "        # this completes the equation 4 and 5\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "        assert input_gate.shape[2] == forget_gate.shape[2]\n",
    "\n",
    "        # to be used for equation 7\n",
    "        input_gate = torch.sigmoid(input_gate + self.input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "    def attend_over_memory(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multiheaded attention over `memory`.\n",
    "            Args:\n",
    "              memory: Current relational memory.\n",
    "            Returns:\n",
    "              The attended-over memory.\n",
    "        \"\"\"\n",
    "        for _ in range(self.num_blocks):\n",
    "            attended_memory = self.multihead_attention(memory)\n",
    "\n",
    "            # Add a skip connection to the multiheaded attention's input.\n",
    "            memory = self.attended_memory_layernorm(memory + attended_memory)\n",
    "\n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            attention_mlp = memory\n",
    "            for i, l in enumerate(self.attention_mlp):\n",
    "                attention_mlp = self.attention_mlp[i](attention_mlp)\n",
    "                attention_mlp = F.relu(attention_mlp)\n",
    "            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n",
    "\n",
    "        return memory\n",
    "\n",
    "    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        \"\"\"\n",
    "        Forward step of the relational memory core.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: Memory output from the previous time step.\n",
    "          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n",
    "            of matrices. Default to False, in which case the input is flattened\n",
    "            into a vector.\n",
    "        Returns:\n",
    "          output: This time step's output.\n",
    "          next_memory: The next version of memory to use.\n",
    "        \"\"\"\n",
    "\n",
    "        if treat_input_as_matrix:\n",
    "            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n",
    "            inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n",
    "            # apply linear layer for dim 2\n",
    "            inputs_reshape = self.input_projector(inputs)\n",
    "        else:\n",
    "            # keep (Batch, ...) dim (0), flatten starting from dim 1\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # apply linear layer for dim 1\n",
    "            inputs = self.input_projector(inputs)\n",
    "            # unsqueeze the time step to dim 1\n",
    "            inputs_reshape = inputs.unsqueeze(dim=1)\n",
    "\n",
    "        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "        next_memory = self.attend_over_memory(memory_plus_input)\n",
    "\n",
    "        # cut out the concatenated input vectors from the original memory slots\n",
    "        n = inputs_reshape.shape[1]\n",
    "        next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "        if self.gate_style == 'unit' or self.gate_style == 'memory':\n",
    "            # these gates are sigmoid-applied ones for equation 7\n",
    "            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n",
    "            # equation 7 calculation\n",
    "            next_memory = input_gate * torch.tanh(next_memory)\n",
    "            next_memory += forget_gate * memory\n",
    "\n",
    "        output = next_memory.view(next_memory.shape[0], -1)\n",
    "\n",
    "        return output, next_memory\n",
    "\n",
    "    def forward(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "\n",
    "        # for loop implementation of (entire) recurrent forward pass of the model\n",
    "        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n",
    "        # so the concatenated logits are [seq * batch, vocab]\n",
    "\n",
    "        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n",
    "\n",
    "        # memory = self.repackage_hidden(memory)\n",
    "        logit = 0\n",
    "        logits = []\n",
    "        # shape[1] is seq_lenth T\n",
    "        for idx_step in range(inputs.shape[1]):\n",
    "            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n",
    "            logits.append(logit.unsqueeze(1))\n",
    "        logits = torch.cat(logits, dim=1)\n",
    "\n",
    "        if self.return_all_outputs:\n",
    "            return logits, memory\n",
    "        else:\n",
    "            return logit.unsqueeze(1), memory\n",
    "\n",
    "# ########## DEBUG: unit test code ##########\n",
    "# input_size = 32\n",
    "# seq_length = 20\n",
    "# batch_size = 32\n",
    "# num_tokens = 5000\n",
    "# model = RelationalMemory(mem_slots=1, head_size=512, input_size=input_size, num_heads=2)\n",
    "# model_memory = model.initial_state(batch_size=batch_size)\n",
    "#\n",
    "# # random input\n",
    "# random_input = torch.randn((32, seq_length, input_size))\n",
    "# # random targets\n",
    "# random_targets = torch.randn((32, seq_length, input_size))\n",
    "#\n",
    "# # take a one step forward\n",
    "# logit, next_memory = model(random_input, model_memory)\n",
    "# print(next_memory.shape)\n",
    "# print(logit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture (LSTM/RMC Generator)\n",
    "class RelGAN(nn.Module):\n",
    "    def __init__(self, mem_slots, num_heads, head_size, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, device=DEVICE):    \n",
    "        super(RelGAN, self).__init__()\n",
    "\n",
    "        # ---------------- #\n",
    "        # model properties #\n",
    "        # ---------------- #\n",
    "        self.name = 'RelGAN'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "        self.mem_slots = mem_slots\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.temperature = 1.0\n",
    "        self.theta = None\n",
    "        self.device = device\n",
    "\n",
    "        # ------------ #\n",
    "        # model layers #\n",
    "        # ------------ #\n",
    "        # LSTM\n",
    "        # self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        # self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # RMC\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.hidden_size = mem_slots * num_heads * head_size\n",
    "        self.lstm = RelationalMemory(mem_slots=mem_slots, head_size=head_size, input_size=embedding_dim,\n",
    "                                     num_heads=num_heads, return_all_outputs=True)\n",
    "        self.lstm2out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, h, need_hidden=False):\n",
    "        emb = self.embeddings(x)\n",
    "        if len(x.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim        \n",
    "        out, hidden = self.lstm(emb, h)  # out: batch_size * seq_len * hidden_dim        \n",
    "        out = out.contiguous().view(-1, self.hidden_size)  # out: (batch_size * len) * hidden_dim        \n",
    "        out = self.lstm2out(out)  # batch_size * seq_len * vocab_size        \n",
    "        out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)        \n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "\n",
    "    def step(self, x, h):\n",
    "        '''\n",
    "        RelGAN step forward\n",
    "        :param inp: [batch_size]\n",
    "        :param hidden: memory size\n",
    "        :return: pred, hidden, next_token, next_token_onehot, next_o\n",
    "            - pred: batch_size * vocab_size, use for adversarial training backward\n",
    "            - hidden: next hidden\n",
    "            - next_token: [batch_size], next sentence token\n",
    "            - next_token_onehot: batch_size * vocab_size, not used yet\n",
    "            - next_o: batch_size * vocab_size, not used yet\n",
    "        '''        \n",
    "        emb = self.embeddings(x).unsqueeze(1)\n",
    "        out, hidden = self.lstm(emb, h)\n",
    "        gumbel_t = self.add_gumbel(self.lstm2out(out.squeeze(1)))\n",
    "        next_token = torch.argmax(gumbel_t, dim=1).detach()\n",
    "        # next_token_onehot = F.one_hot(next_token, cfg.vocab_size).float()  # not used yet\n",
    "        next_token_onehot = None\n",
    "\n",
    "        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n",
    "        # next_o = torch.sum(next_token_onehot * pred, dim=1)  # not used yet\n",
    "        next_o = None\n",
    "\n",
    "        return pred, hidden, next_token, next_token_onehot, next_o\n",
    "\n",
    "\n",
    "    def sample(self, num_samples, batch_size, one_hot=False, start_letter=1):\n",
    "        \"\"\"\n",
    "        Sample from RelGAN Generator\n",
    "        - one_hot: if return pred of RelGAN, used for adversarial training\n",
    "        :return:\n",
    "            - all_preds: batch_size * seq_len * vocab_size, only use for a batch\n",
    "            - samples: all samples\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len, device=self.device).long()\n",
    "        if one_hot:\n",
    "            all_preds = torch.zeros(batch_size, self.max_seq_len, self.vocab_size, device=self.device)\n",
    "\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size).to(self.device)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size).to(self.device)            \n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                pred, hidden, next_token, _, _ = self.step(inp, hidden)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token\n",
    "                if one_hot:\n",
    "                    all_preds[:, i] = pred\n",
    "                inp = next_token\n",
    "        samples = samples[:num_samples]  # num_samples * seq_len\n",
    "\n",
    "        if one_hot:\n",
    "            return all_preds  # batch_size * seq_len * vocab_size\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size=BATCH_SIZE):\n",
    "        \"\"\"init RMC memory\"\"\"\n",
    "        memory = self.lstm.initial_state(batch_size)\n",
    "        memory = self.lstm.repackage_hidden(memory)  # detch memory at first        \n",
    "        return memory.to(self.device)\n",
    "\n",
    "    \n",
    "    def add_gumbel(self, o_t, eps=1e-10):\n",
    "        \"\"\"Add o_t by a vector sampled from Gumbel(0,1)\"\"\"\n",
    "        u = torch.zeros(o_t.size(), device=self.device)\n",
    "        \n",
    "        u.uniform_(0, 1)\n",
    "        g_t = -torch.log(-torch.log(u + eps) + eps)\n",
    "        gumbel_t = o_t + g_t\n",
    "        return gumbel_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_SLOTS = 1\n",
    "NUM_HEADS = 2\n",
    "HEAD_SIZE = 256\n",
    "GEN_EMBED_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RelGAN(MEM_SLOTS, NUM_HEADS, HEAD_SIZE, embedding_dim=GEN_EMBED_DIM, \n",
    "             hidden_dim=GEN_HIDDEN_DIM, vocab_size=VOCAB_SIZE, \n",
    "             max_seq_len=MAX_SEQ_LEN, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture discriminator\n",
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len, num_rep, vocab_size, padding_idx,\n",
    "                 filter_sizes, num_filters, dropout=0.25, device=DEVICE):\n",
    "    # def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "\n",
    "        # ---------------- #\n",
    "        # model properties #\n",
    "        # ---------------- #\n",
    "        self.name = 'CNNDiscriminator'\n",
    "        self.embed_size = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.emb_dim_single = int(embed_dim / num_rep)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx        \n",
    "        \n",
    "        # ------------ #\n",
    "        # model layers #\n",
    "        # ------------ #\n",
    "        self.embeddings = nn.Linear(vocab_size, embed_dim, bias=False)        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, self.emb_dim_single), stride=(1, self.emb_dim_single)) for (n, f) in \n",
    "            zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 100)\n",
    "        self.out2logits = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embeddings(x).unsqueeze(1)\n",
    "\n",
    "        convs = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pools = [F.max_pool2d(con, (con.size(2), 1)).squeeze(2) for con in convs]\n",
    "        pred = torch.cat(pools, 1)\n",
    "        pred = pred.permute(0, 2, 1).contiguous().view(-1, self.feature_dim)\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred\n",
    "\n",
    "        pred = self.feature2out(self.dropout(pred))\n",
    "        logits = self.out2logits(pred).squeeze(1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIS_EMBED_DIM = 64\n",
    "NUM_REP = 64\n",
    "\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "num_filters = [300, 300, 300, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = CNNDiscriminator(DIS_EMBED_DIM, MAX_SEQ_LEN, NUM_REP, VOCAB_SIZE, padding_idx=2,\n",
    "                        filter_sizes=filter_sizes, num_filters=num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded.\n"
     ]
    }
   ],
   "source": [
    "# if there is pretrained model, then continue training\n",
    "weights = torch.load(f'../models/gan/pretrain_{gen.name}_coco.pt', map_location=DEVICE)\n",
    "gen.load_state_dict(weights)\n",
    "print('Weights loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c763535d8d540eda6069a42980aacaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150 | Loss: 2.6736169159412384\n",
      "Epoch: 2/150 | Loss: 1.2775412529706955\n",
      "Epoch: 3/150 | Loss: 1.1268819630146027\n",
      "Epoch: 4/150 | Loss: 1.0231304958462715\n",
      "Epoch: 5/150 | Loss: 0.9521785035729409\n",
      "Epoch: 6/150 | Loss: 0.8955184251070023\n",
      "Epoch: 7/150 | Loss: 0.8486252546310424\n",
      "Epoch: 8/150 | Loss: 0.8180998906493187\n",
      "Epoch: 9/150 | Loss: 0.7852315351366996\n",
      "Epoch: 10/150 | Loss: 0.7559637054800987\n",
      "Epoch: 11/150 | Loss: 0.7376291051506996\n",
      "Epoch: 12/150 | Loss: 0.7124564543366432\n",
      "Epoch: 13/150 | Loss: 0.6919564589858055\n",
      "Epoch: 14/150 | Loss: 0.680003148317337\n",
      "Epoch: 15/150 | Loss: 0.669129666686058\n",
      "Epoch: 16/150 | Loss: 0.6501296043395997\n",
      "Epoch: 17/150 | Loss: 0.6319865137338638\n",
      "Epoch: 18/150 | Loss: 0.6244554445147514\n",
      "Epoch: 19/150 | Loss: 0.6099600702524185\n",
      "Epoch: 20/150 | Loss: 0.599637983739376\n",
      "Epoch: 21/150 | Loss: 0.5890483587980271\n",
      "Epoch: 22/150 | Loss: 0.5815408945083618\n",
      "Epoch: 23/150 | Loss: 0.5741306453943252\n",
      "Epoch: 24/150 | Loss: 0.5667961776256562\n",
      "Epoch: 25/150 | Loss: 0.5534284994006157\n",
      "Epoch: 26/150 | Loss: 0.5468119382858276\n",
      "Epoch: 27/150 | Loss: 0.5391798578202724\n",
      "Epoch: 28/150 | Loss: 0.5294812105596065\n",
      "Epoch: 29/150 | Loss: 0.5137431271374225\n",
      "Epoch: 30/150 | Loss: 0.5143884226679802\n",
      "[WARN] Loss didn't improving (0.5137 --> 0.5144)\n",
      "Epoch: 31/150 | Loss: 0.5133142799139023\n",
      "Epoch: 32/150 | Loss: 0.5012254193425179\n",
      "Epoch: 33/150 | Loss: 0.4953018181025982\n",
      "Epoch: 34/150 | Loss: 0.4821350954473019\n",
      "Epoch: 35/150 | Loss: 0.47530081793665885\n",
      "Epoch: 36/150 | Loss: 0.47339769527316095\n",
      "Epoch: 37/150 | Loss: 0.46788153275847433\n",
      "Epoch: 38/150 | Loss: 0.45815728306770326\n",
      "Epoch: 39/150 | Loss: 0.44894780218601227\n",
      "Epoch: 40/150 | Loss: 0.449528419226408\n",
      "[WARN] Loss didn't improving (0.4489 --> 0.4495)\n",
      "Epoch: 41/150 | Loss: 0.4504045695066452\n",
      "[WARN] Loss didn't improving (0.4489 --> 0.4504)\n",
      "Epoch: 42/150 | Loss: 0.4476060293614864\n",
      "Epoch: 43/150 | Loss: 0.4467921756207943\n",
      "Epoch: 44/150 | Loss: 0.4423769094049931\n",
      "Epoch: 45/150 | Loss: 0.4339949108660221\n",
      "Epoch: 46/150 | Loss: 0.4284153588116169\n",
      "Epoch: 47/150 | Loss: 0.425089143961668\n",
      "Epoch: 48/150 | Loss: 0.42351156324148176\n",
      "Epoch: 49/150 | Loss: 0.4177848696708679\n",
      "Epoch: 50/150 | Loss: 0.4121173799037933\n",
      "Epoch: 51/150 | Loss: 0.4090236000716686\n",
      "Epoch: 52/150 | Loss: 0.4003626637160778\n",
      "Epoch: 53/150 | Loss: 0.39962382018566134\n",
      "Epoch: 54/150 | Loss: 0.3971270598471165\n",
      "Epoch: 55/150 | Loss: 0.4024942867457867\n",
      "[WARN] Loss didn't improving (0.3971 --> 0.4025)\n",
      "Epoch: 56/150 | Loss: 0.3937786392867565\n",
      "Epoch: 57/150 | Loss: 0.39100857451558113\n",
      "Epoch: 58/150 | Loss: 0.38603067100048066\n",
      "Epoch: 59/150 | Loss: 0.38045838922262193\n",
      "Epoch: 60/150 | Loss: 0.3823096387088299\n",
      "[WARN] Loss didn't improving (0.3805 --> 0.3823)\n",
      "Epoch: 61/150 | Loss: 0.37746511623263357\n",
      "Epoch: 62/150 | Loss: 0.38393178060650823\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.3839)\n",
      "Epoch: 63/150 | Loss: 0.4038021430373192\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.4038)\n",
      "Epoch: 64/150 | Loss: 0.3925711713731289\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.3926)\n",
      "Epoch: 65/150 | Loss: 0.3857911892235279\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.3858)\n",
      "Epoch: 66/150 | Loss: 0.3828071177005768\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.3828)\n",
      "Epoch: 67/150 | Loss: 0.38493502140045166\n",
      "[WARN] Loss didn't improving (0.3775 --> 0.3849)\n",
      "Epoch: 68/150 | Loss: 0.374602260440588\n",
      "Epoch: 69/150 | Loss: 0.3745103321969509\n",
      "Epoch: 70/150 | Loss: 0.4077250465750694\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.4077)\n",
      "Epoch: 71/150 | Loss: 0.44876593351364136\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.4488)\n",
      "Epoch: 72/150 | Loss: 0.4160952247679234\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.4161)\n",
      "Epoch: 73/150 | Loss: 0.3999664880335331\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.4000)\n",
      "Epoch: 74/150 | Loss: 0.39289825558662417\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.3929)\n",
      "Epoch: 75/150 | Loss: 0.37902872264385223\n",
      "[WARN] Loss didn't improving (0.3745 --> 0.3790)\n",
      "Epoch: 76/150 | Loss: 0.3705007411539555\n",
      "Epoch: 77/150 | Loss: 0.36533929258584974\n",
      "Epoch: 78/150 | Loss: 0.35793676525354384\n",
      "Epoch: 79/150 | Loss: 0.3590027645230293\n",
      "[WARN] Loss didn't improving (0.3579 --> 0.3590)\n",
      "Epoch: 80/150 | Loss: 0.3602251045405865\n",
      "[WARN] Loss didn't improving (0.3579 --> 0.3602)\n",
      "Epoch: 81/150 | Loss: 0.3663699015974998\n",
      "[WARN] Loss didn't improving (0.3579 --> 0.3664)\n",
      "Epoch: 82/150 | Loss: 0.35789328515529634\n",
      "Epoch: 83/150 | Loss: 0.35617452710866926\n",
      "Epoch: 84/150 | Loss: 0.3598918363451958\n",
      "[WARN] Loss didn't improving (0.3562 --> 0.3599)\n",
      "Epoch: 85/150 | Loss: 0.3557040773332119\n",
      "Epoch: 86/150 | Loss: 0.35784459188580514\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3578)\n",
      "Epoch: 87/150 | Loss: 0.3575609415769577\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3576)\n",
      "Epoch: 88/150 | Loss: 0.3586316518485546\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3586)\n",
      "Epoch: 89/150 | Loss: 0.36704074814915655\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3670)\n",
      "Epoch: 90/150 | Loss: 0.3624011009931564\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3624)\n",
      "Epoch: 91/150 | Loss: 0.35620642825961113\n",
      "[WARN] Loss didn't improving (0.3557 --> 0.3562)\n",
      "Epoch: 92/150 | Loss: 0.3511492840945721\n",
      "Epoch: 93/150 | Loss: 0.35165072679519654\n",
      "[WARN] Loss didn't improving (0.3511 --> 0.3517)\n",
      "Epoch: 94/150 | Loss: 0.3489760093390942\n",
      "Epoch: 95/150 | Loss: 0.3471990093588829\n",
      "Epoch: 96/150 | Loss: 0.3529993645846844\n",
      "[WARN] Loss didn't improving (0.3472 --> 0.3530)\n",
      "Epoch: 97/150 | Loss: 0.3510442622005939\n",
      "[WARN] Loss didn't improving (0.3472 --> 0.3510)\n",
      "Epoch: 98/150 | Loss: 0.3531100109219551\n",
      "[WARN] Loss didn't improving (0.3472 --> 0.3531)\n",
      "Epoch: 99/150 | Loss: 0.3456129692494869\n",
      "Epoch: 100/150 | Loss: 0.34750784039497373\n",
      "[WARN] Loss didn't improving (0.3456 --> 0.3475)\n",
      "Epoch: 101/150 | Loss: 0.3401845693588257\n",
      "Epoch: 102/150 | Loss: 0.3316724091768265\n",
      "Epoch: 103/150 | Loss: 0.3359113700687885\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3359)\n",
      "Epoch: 104/150 | Loss: 0.33701092973351476\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3370)\n",
      "Epoch: 105/150 | Loss: 0.3389075383543968\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3389)\n",
      "Epoch: 106/150 | Loss: 0.3428586110472679\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3429)\n",
      "Epoch: 107/150 | Loss: 0.3441152274608612\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3441)\n",
      "Epoch: 108/150 | Loss: 0.3405286230146885\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3405)\n",
      "Epoch: 109/150 | Loss: 0.3478079192340374\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3478)\n",
      "Epoch: 110/150 | Loss: 0.3443718753755093\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3444)\n",
      "Epoch: 111/150 | Loss: 0.3381608225405216\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3382)\n",
      "Epoch: 112/150 | Loss: 0.336725153028965\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3367)\n",
      "Epoch: 113/150 | Loss: 0.340505201369524\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3405)\n",
      "Epoch: 114/150 | Loss: 0.34232613891363145\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3423)\n",
      "Epoch: 115/150 | Loss: 0.33858274668455124\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3386)\n",
      "Epoch: 116/150 | Loss: 0.33251427859067917\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3325)\n",
      "Epoch: 117/150 | Loss: 0.3325305536389351\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3325)\n",
      "Epoch: 118/150 | Loss: 0.33967009410262106\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3397)\n",
      "Epoch: 119/150 | Loss: 0.3481682263314724\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3482)\n",
      "Epoch: 120/150 | Loss: 0.3511579059064388\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3512)\n",
      "Epoch: 121/150 | Loss: 0.34930098354816436\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3493)\n",
      "Epoch: 122/150 | Loss: 0.34632017016410827\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3463)\n",
      "Epoch: 123/150 | Loss: 0.34688073620200155\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3469)\n",
      "Epoch: 124/150 | Loss: 0.34571258127689364\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3457)\n",
      "Epoch: 125/150 | Loss: 0.3496217019855976\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3496)\n",
      "Epoch: 126/150 | Loss: 0.34044979214668275\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3404)\n",
      "Epoch: 127/150 | Loss: 0.3436016447842121\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3436)\n",
      "Epoch: 128/150 | Loss: 0.34480032697319984\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3448)\n",
      "Epoch: 129/150 | Loss: 0.3402522198855877\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3403)\n",
      "Epoch: 130/150 | Loss: 0.33611026406288147\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3361)\n",
      "Epoch: 131/150 | Loss: 0.33531455248594283\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3353)\n",
      "Epoch: 132/150 | Loss: 0.33314520716667173\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3331)\n",
      "Epoch: 133/150 | Loss: 0.33502482399344446\n",
      "[WARN] Loss didn't improving (0.3317 --> 0.3350)\n",
      "Epoch: 134/150 | Loss: 0.33165294453501704\n",
      "Epoch: 135/150 | Loss: 0.32885091155767443\n",
      "Epoch: 136/150 | Loss: 0.32698175832629206\n",
      "Epoch: 137/150 | Loss: 0.32218688577413557\n",
      "Epoch: 138/150 | Loss: 0.3276525601744652\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3277)\n",
      "Epoch: 139/150 | Loss: 0.32802123576402664\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3280)\n",
      "Epoch: 140/150 | Loss: 0.3243610166013241\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3244)\n",
      "Epoch: 141/150 | Loss: 0.3272425390779972\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3272)\n",
      "Epoch: 142/150 | Loss: 0.3350532531738281\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3351)\n",
      "Epoch: 143/150 | Loss: 0.33866908103227616\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3387)\n",
      "Epoch: 144/150 | Loss: 0.3407244339585304\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3407)\n",
      "Epoch: 145/150 | Loss: 0.3433924578130245\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3434)\n",
      "Epoch: 146/150 | Loss: 0.3397788770496845\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3398)\n",
      "Epoch: 147/150 | Loss: 0.339782840013504\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3398)\n",
      "Epoch: 148/150 | Loss: 0.33875108808279036\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3388)\n",
      "Epoch: 149/150 | Loss: 0.3430086329579353\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3430)\n",
      "Epoch: 150/150 | Loss: 0.34603420495986936\n",
      "[WARN] Loss didn't improving (0.3222 --> 0.3460)\n"
     ]
    }
   ],
   "source": [
    "# pretraining generator\n",
    "gen_criterion = nn.NLLLoss()\n",
    "gen_optim = Adam(gen.parameters(), lr=0.01)\n",
    "gen_pretrain_epochs = 150\n",
    "gen_clip_norm = 5.0\n",
    "gen_loss_min = torch.inf\n",
    "\n",
    "gen.to(DEVICE)\n",
    "\n",
    "epochloop = tqdm(range(gen_pretrain_epochs), position=0, desc='Training...', leave=True)\n",
    "\n",
    "for e in epochloop:\n",
    "    \n",
    "    gen.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(genloader):\n",
    "        feature, target = batch['input'].to(DEVICE), batch['target'].to(DEVICE)\n",
    "        hidden = gen.init_hidden(batch_size=len(feature)).to(DEVICE)    # use length of feature as batch_size to handle different last batch total item\n",
    "\n",
    "        pred = gen(feature, hidden)\n",
    "\n",
    "        loss = gen_criterion(pred, target.view(-1))\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), gen_clip_norm)\n",
    "        gen_optim.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        epochloop.set_postfix_str(f'Batch: {i+1}/{len(genloader)} | Loss: {train_loss/(i+1):.3f}')\n",
    "\n",
    "    avg_loss = train_loss / len(genloader)\n",
    "\n",
    "    print(f'Epoch: {e+1}/{gen_pretrain_epochs} | Loss: {avg_loss}')\n",
    "    if avg_loss <= gen_loss_min:\n",
    "        torch.save(gen.state_dict(), f'../models/gan/pretrain_{gen.name}_coco.pt')\n",
    "        gen_loss_min = avg_loss\n",
    "    else:\n",
    "        print(f'[WARN] Loss didn\\'t improving ({gen_loss_min:.4f} --> {avg_loss:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def get_losses(d_out_real, d_out_fake, loss_type='JS'):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if loss_type == 'standard':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = bce_loss(d_out_fake, torch.ones_like(d_out_fake))\n",
    "\n",
    "    elif loss_type == 'JS':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -d_loss_fake\n",
    "\n",
    "    elif loss_type == 'KL':\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = torch.mean(-d_out_fake)\n",
    "\n",
    "    elif loss_type == 'hinge':\n",
    "        d_loss_real = torch.mean(nn.ReLU(1.0 - d_out_real))\n",
    "        d_loss_fake = torch.mean(nn.ReLU(1,0 + d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -torch.mean(d_out_fake)\n",
    "\n",
    "    elif loss_type == 'rsgan':\n",
    "        d_loss = bce_loss(d_out_real - d_out_fake, torch.ones_like(d_out_real))\n",
    "        g_loss = bce_loss(d_out_fake - d_out_real, torch.ones_like(d_out_fake))\n",
    "\n",
    "    return g_loss, d_loss\n",
    "\n",
    "\n",
    "def get_fixed_temperature(temper, i, N, adapt='exp'):\n",
    "    \"\"\"A function to set up different temperature control policies\"\"\"\n",
    "    N = 5000\n",
    "\n",
    "    if adapt == 'no':\n",
    "        temper_var_np = 1.0  # no increase, origin: temper\n",
    "    elif adapt == 'lin':\n",
    "        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n",
    "    elif adapt == 'exp':\n",
    "        temper_var_np = temper ** (i / N)  # exponential increase\n",
    "    elif adapt == 'log':\n",
    "        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n",
    "    elif adapt == 'sigmoid':\n",
    "        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n",
    "    elif adapt == 'quad':\n",
    "        temper_var_np = (temper - 1) / (N - 1) ** 2 * i ** 2 + 1\n",
    "    elif adapt == 'sqrt':\n",
    "        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown adapt type!\")\n",
    "\n",
    "    return temper_var_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(gen, num_sample=10000, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        eval_sample = gen.sample(num_sample, 4 * batch_size)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd4f4da00e3406fa736daf0fcaef94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADV] Epoch: 99/2000 | g_loss: 9.4535, d_loss: 0.0018, temperature: 1.0\n",
      "[ADV] Epoch: 199/2000 | g_loss: 10.8617, d_loss: 0.0041, temperature: 1.0\n",
      "[ADV] Epoch: 299/2000 | g_loss: 14.6961, d_loss: 0.0030, temperature: 1.0\n",
      "[ADV] Epoch: 399/2000 | g_loss: 9.4857, d_loss: 0.0289, temperature: 1.0\n",
      "[ADV] Epoch: 499/2000 | g_loss: 10.3538, d_loss: 0.0150, temperature: 1.0\n",
      "[ADV] Epoch: 599/2000 | g_loss: 13.5506, d_loss: 0.0006, temperature: 1.0\n",
      "[ADV] Epoch: 699/2000 | g_loss: 15.3665, d_loss: 0.0018, temperature: 1.0\n",
      "[ADV] Epoch: 799/2000 | g_loss: 17.0717, d_loss: 0.0012, temperature: 1.0\n",
      "[ADV] Epoch: 899/2000 | g_loss: 19.2364, d_loss: 0.0006, temperature: 1.0\n",
      "[ADV] Epoch: 999/2000 | g_loss: 19.0016, d_loss: 0.0006, temperature: 1.0\n",
      "[ADV] Epoch: 1099/2000 | g_loss: 17.1816, d_loss: 0.0006, temperature: 1.0\n",
      "[ADV] Epoch: 1199/2000 | g_loss: 22.3603, d_loss: 0.0007, temperature: 1.0\n",
      "[ADV] Epoch: 1299/2000 | g_loss: 23.5356, d_loss: 0.0005, temperature: 1.0\n",
      "[ADV] Epoch: 1399/2000 | g_loss: 26.1701, d_loss: 0.0001, temperature: 1.0\n",
      "[ADV] Epoch: 1499/2000 | g_loss: 27.9926, d_loss: 0.0001, temperature: 1.0\n",
      "[ADV] Epoch: 1599/2000 | g_loss: 29.3962, d_loss: 0.0005, temperature: 1.0\n",
      "[ADV] Epoch: 1699/2000 | g_loss: 28.1545, d_loss: 0.0001, temperature: 1.0\n",
      "[ADV] Epoch: 1799/2000 | g_loss: 33.1698, d_loss: 0.0000, temperature: 1.0\n",
      "[ADV] Epoch: 1899/2000 | g_loss: 35.7473, d_loss: 0.0000, temperature: 1.0\n",
      "[ADV] Epoch: 1999/2000 | g_loss: 37.5014, d_loss: 0.0000, temperature: 1.0\n"
     ]
    }
   ],
   "source": [
    "# adversarial training\n",
    "adv_train_epoch = 2000\n",
    "adv_gen_step = 1\n",
    "adv_disc_step = 5\n",
    "gen_adv_optim = Adam(gen.parameters(), lr=1e-4)\n",
    "disc_adv_optim = Adam(disc.parameters(), lr=1e-4)\n",
    "adv_clip_norm = 5.0\n",
    "loss_type = 'rsgan'\n",
    "\n",
    "adv_epochloop = tqdm(range(adv_train_epoch))\n",
    "\n",
    "for e in adv_epochloop:\n",
    "    # adv train generator\n",
    "    adv_gen_loss = 0\n",
    "    for i in range(adv_gen_step):\n",
    "        # get random real sample\n",
    "        real_sample = get_random_batch(genloader)['target'].to(DEVICE)\n",
    "        gen_sample = gen.sample(BATCH_SIZE, BATCH_SIZE, one_hot=True).to(DEVICE)\n",
    "        real_sample = F.one_hot(real_sample, VOCAB_SIZE).float()\n",
    "\n",
    "        # train\n",
    "        d_out_real = disc(real_sample)\n",
    "        d_out_fake = disc(gen_sample)\n",
    "        g_loss, _ = get_losses(d_out_real, d_out_fake, loss_type=loss_type)\n",
    "\n",
    "        # optimize\n",
    "        gen_adv_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), adv_clip_norm)\n",
    "        gen_adv_optim.step()\n",
    "        adv_gen_loss += g_loss.item()\n",
    "\n",
    "        adv_epochloop.set_description(f'Generator step: {i+1}/{adv_gen_step} | g_loss: {(adv_gen_loss/(i+1)):.3f}')        \n",
    "\n",
    "    adv_gen_loss = adv_gen_loss / adv_gen_step if adv_gen_step != 0 else 0\n",
    "\n",
    "    # adv train discriminator\n",
    "    adv_disc_loss = 0\n",
    "    for i in range(adv_disc_step):\n",
    "        # get random real sample\n",
    "        real_sample = get_random_batch(genloader)['target'].to(DEVICE)\n",
    "        gen_sample = gen.sample(BATCH_SIZE, BATCH_SIZE, one_hot=True).to(DEVICE)\n",
    "        real_sample = F.one_hot(real_sample, VOCAB_SIZE).float()\n",
    "\n",
    "        # train\n",
    "        d_out_real = disc(real_sample)\n",
    "        d_out_fake = disc(gen_sample)\n",
    "        _, d_loss = get_losses(d_out_real, d_out_fake, loss_type=loss_type)\n",
    "\n",
    "        # optimize\n",
    "        disc_adv_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(disc.parameters(), adv_clip_norm)\n",
    "        disc_adv_optim.step()\n",
    "        adv_disc_loss += d_loss.item()\n",
    "\n",
    "        adv_epochloop.set_description(f'Discriminator step: {i+1}/{adv_disc_step} | d_loss: {(adv_disc_loss/(i+1)):.3f}')\n",
    "\n",
    "    adv_disc_loss = adv_disc_loss / adv_disc_step if adv_disc_step != 0 else 0\n",
    "\n",
    "    # update generator temperature\n",
    "    gen.temperature = get_fixed_temperature(1, e, adv_train_epoch, adapt='exp')    \n",
    "    \n",
    "    if (e+1) % 100 == 0:\n",
    "        print(f'[ADV] Epoch: {e}/{adv_train_epoch} | g_loss: {adv_gen_loss:.4f}, d_loss: {adv_disc_loss:.4f}, temperature: {gen.temperature}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8564a27cb82d73423f8ef7649afe412fe88be26d8d7a10840ebe1fcfca8dcfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
