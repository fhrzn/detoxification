{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/raw/jigsaw/jigsaw-multiling-1st-subset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"',\n",
       "       '\"\\nFair use rationale for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair use template, you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use.\\n\\nPlease go to the image description page and edit it to include a fair use rationale.\\n\\nIf you have uploaded other fair use media, consider checking that you have specified the fair use rationale on those pages too. You can find a list of \\'image\\' pages you have edited by clicking on the \"\"my contributions\"\" link (it is located at the very top of any Wikipedia page when you are logged in), and then selecting \"\"Image\"\" from the dropdown box. Note that any fair use images uploaded after 4 May, 2006, and lacking such an explanation will be deleted one week after they have been uploaded, as described on criteria for speedy deletion. If you have any questions please ask them at the Media copyright questions page. Thank you. (talk • contribs • ) \\nUnspecified source for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I noticed that the file\\'s description page currently doesn\\'t specify who created the content, so the copyright status is unclear. If you did not create this file yourself, then you will need to specify the owner of the copyright. If you obtained it from a website, then a link to the website from which it was taken, together with a restatement of that website\\'s terms of use of its content, is usually sufficient information. However, if the copyright holder is different from the website\\'s publisher, then their copyright should also be acknowledged.\\n\\nAs well as adding the source, please add a proper copyright licensing tag if the file doesn\\'t have one already. If you created/took the picture, audio, or video then the  tag can be used to release it under the GFDL. If you believe the media meets the criteria at Wikipedia:Fair use, use a tag such as  or one of the other tags listed at Wikipedia:Image copyright tags#Fair use. See Wikipedia:Image copyright tags for the full list of copyright tags that you can use.\\n\\nIf you have uploaded other files, consider checking that you have specified their source and tagged them, too. You can find a list of files you have uploaded by following [ this link]. Unsourced and untagged images may be deleted one week after they have been tagged, as described on criteria for speedy deletion. If the image is copyrighted under a non-free license (per Wikipedia:Fair use) then the image will be deleted 48 hours after . If you have any questions please ask them at the Media copyright questions page. Thank you. (talk • contribs • ) \"',\n",
       "       '\"\\n\\n Snowflakes are NOT always symmetrical! \\n\\nUnder Geometry it is stated that \"\"A snowflake always has six symmetric arms.\"\" This assertion is simply not true! According to Kenneth Libbrecht, \"\"The rather unattractive irregular crystals are by far the most common variety.\"\" http://www.its.caltech.edu/~atomic/snowcrystals/myths/myths.htm#perfection Someone really need to take a look at his site and get FACTS off of it because I still see a decent number of falsities on this page. (forgive me Im new at this and dont want to edit anything)\"',\n",
       "       ...,\n",
       "       \"== Wikipedia talk:Featured article candidates#Consensus and (archived) FACs == \\n\\n Nancy, I archived the off-topic portion of the discussion at the above FAC talk thread. Just to clarify, I don't have a horse in this race, but the back-and-forth discussion between you and others was escalating at an alarming pace. I want to assure you that I am neutral, do not hold a grudge or any ill feelings against you, and have no opinion on the subject of discussion. Hopefully the issue can be resolved, but perhaps the article's talk page or a user talk page is a better place. Cheers,\",\n",
       "       '\" \\n\\n  \\n == Merging with \"\"Impact of technology on the educational system\"\" == \\n\\n Hello, Eric. I am picking up a thread you started at Talk:Educational technology# \\n\\n Merging with \"\"Impact of technology on the educational system\"\" on 24 April 2012 (UTC). Do I correctly mirror the points you posted by restating things as, \"\"Educational technology\"\" properly considered is a domain of \"\"learning theory\"\" and is not to be confused with the use of \"\"hardware and software\"\" within education?  Responding either here or on the said Talk Page will help me confirm my understanding.   \"',\n",
       "       '== Wikipedia:Miscellany for deletion/Secret pages 2 == \\n\\n Because you participated in Wikipedia talk:What Wikipedia is not/Archive 34#Does WP:NOTMYSPACE apply to secret pages?, you may be interested in Wikipedia:Miscellany for deletion/Secret pages 2.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['comment_text'].str.contains('#')].comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "def rm_ip_address(text):\n",
    "    return re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
    "\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "def rm_inappropriate_sym(text):\n",
    "    return re.sub(r'[\\:\\%\\=\\~]', ' ', text)\n",
    "\n",
    "def rm_money(text):\n",
    "    return re.sub(r'\\$\\s?((?:\\d+[A-z])|((?:\\d+[\\,\\.])+\\d+(?=\\s))|((?:\\d+)))', r'', text)\n",
    "\n",
    "def space_between_sym(text):    \n",
    "    return re.sub(r'([\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~])', r' \\1 ', text)\n",
    "\n",
    "def rm_additional_space(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "    \n",
    "def rm_email(text):\n",
    "    return re.sub(r'(?:(\\S+)?\\@\\S+)', r'', text)\n",
    "\n",
    "def rm_middle_dot(text):\n",
    "    return re.sub(r'(?<=\\w)\\.(?=\\w+)', '', text)\n",
    "\n",
    "def rm_middle_spaces(text):\n",
    "    return re.sub(r'(?<=\\w)\\s(?=\\w+)', '', text)\n",
    "\n",
    "def clean_pipeline(text):\n",
    "    no_sym = rm_inappropriate_sym(text)\n",
    "    no_ip_address = rm_ip_address(no_sym)\n",
    "    no_link = rm_link(no_ip_address)\n",
    "    no_emoji = rm_emoji(no_link)\n",
    "    no_nonascii = rm_nonascii(no_emoji)\n",
    "    no_email = rm_email(no_nonascii)    \n",
    "    no_mid_dots = rm_middle_dot(no_email)\n",
    "    space_between = space_between_sym(no_mid_dots)\n",
    "    single_space = rm_additional_space(space_between)\n",
    "\n",
    "    return single_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_toxic(row):\n",
    "    return 1 if row.sum() > 1 else -1 if row.sum() < 0 else 0\n",
    "\n",
    "train_df['is_toxic'] = train_df.iloc[:, 2:].progress_apply(is_toxic, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:40<00:00, 5537.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['comment_clean'] = train_df.comment_text.progress_apply(clean_pipeline)\n",
    "# train_df = train_df[['comment_clean', 'is_toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178839, 22355, 22355)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(train_df, test_size=0.2, shuffle=True, stratify=train_df['is_toxic'])\n",
    "test, val = train_test_split(test, test_size=0.5, shuffle=True, stratify=test['is_toxic'])\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to HF dataset format for easy load later\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train),\n",
    "    'validation': Dataset.from_pandas(val),\n",
    "    'test': Dataset.from_pandas(test)\n",
    "})\n",
    "\n",
    "dataset = dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "# dataset.save_to_disk('../data/interim/toxic-cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../data/interim/toxic-cleaned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "\n",
    "    texts = [t.strip() for t in examples['comment_clean']]    \n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_len,\n",
    "        padding='max_length',\n",
    "        stride=10,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = []    \n",
    "\n",
    "    # extract mapping between new and old indices\n",
    "    ### this operation simply populate labels over the overflowing tokens    \n",
    "    sample_map = tokenized.pop('overflow_to_sample_mapping')    \n",
    "    for i in sample_map:\n",
    "        tokenized['labels'].append(examples['is_toxic'][i])\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [00:51<00:00,  3.50ba/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.45ba/s]\n",
      "100%|██████████| 23/23 [00:05<00:00,  3.93ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = DatasetDict({\n",
    "    'train': dataset['train'].map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names),\n",
    "    'validation': dataset['validation'].map(tokenize_data, batched=True, remove_columns=dataset['validation'].column_names),\n",
    "    'test': dataset['test'].map(tokenize_data, batched=True, remove_columns=dataset['test'].column_names)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 382550\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 48330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "flatten_train = tokenized_dataset['train']['input_ids'].view(-1)\n",
    "flatten_val = tokenized_dataset['validation']['input_ids'].view(-1)\n",
    "flatten_test = tokenized_dataset['test']['input_ids'].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_all = list(set(flatten_train.tolist() + flatten_val.tolist() + flatten_test.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:42<00:00, 42.42s/it]\n",
      "Pushing split validation to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.80s/it]\n",
      "Pushing split test to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset.save_to_disk('../data/interim/toxic-cleaned-tokenized')\n",
    "tokenized_dataset.push_to_hub('affahrizain/jigsaw-toxic-comment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
